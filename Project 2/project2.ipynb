{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raghu\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Raghu\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score,log_loss\n",
    "from sklearn.metrics import roc_curve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import save,load,save_plot,plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (1) : Building TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data ==> 7882\n"
     ]
    }
   ],
   "source": [
    "categories = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "twenty_train = fetch_20newsgroups(subset='all', categories=categories, shuffle=True,\n",
    "                                  random_state=42, remove=('headers','footers','quotes'))\n",
    "print(\"Length of training data ==>\", len(twenty_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Report Dimensions:---------------\n",
      "Shape of Training Data ==> (7882, 16564)\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def initParams(tfidf_min_df):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    analyzer = CountVectorizer().build_analyzer()\n",
    "    #Use token_pattern parameter with analyzer='word' if no stemming and want to remove words that are only numbers.\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df=tfidf_min_df, max_df=0.8, analyzer=\"word\",\n",
    "                                 strip_accents='ascii', token_pattern='\\w*[a-zA-Z]')\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    return [stemmer, analyzer,vectorizer,tfidf_transformer]\n",
    "\n",
    "def getTfidf(vectorizer,tfidf_transformer,data,isTraining=True):\n",
    "    if(isTraining):\n",
    "        count_data = vectorizer.fit_transform(data)\n",
    "        tfidf_data = tfidf_transformer.fit_transform(count_data)\n",
    "    else:\n",
    "        count_data = vectorizer.transform(data)\n",
    "        tfidf_data = tfidf_transformer.transform(count_data)        \n",
    "    return tfidf_data\n",
    "\n",
    "def stemmedWords(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "stemmer, analyzer,vectorizer,tfidf_transformer = initParams(tfidf_min_df=3)\n",
    "\n",
    "load_from_previous = True\n",
    "\n",
    "if(load_from_previous):\n",
    "    training_data = load('tfidf_training')\n",
    "else:\n",
    "    training_data = getTfidf(vectorizer,tfidf_transformer,twenty_train.data)\n",
    "    save(training_data,'tfidf_training')\n",
    "print('---------------Report Dimensions:---------------')\n",
    "print('Shape of Training Data ==>', training_data.shape)\n",
    "print('------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART(2): Applying k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bench_k_means(n_clusters, data, name='k-means++'):\n",
    "    t0 = time()\n",
    "    estimator = KMeans(n_clusters=n_clusters, init=name, n_init=10, max_iter=10, tol=0.0001, precompute_distances='auto', \n",
    "                verbose=0, random_state=42, copy_x=True, n_jobs=1, algorithm='auto')\n",
    "    estimator.fit(data)\n",
    "    print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             ))\n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7882,)\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 2\n",
    "#kmeans.fit(training_data)\n",
    "#res = kmeans.predict(testing_data)\n",
    "#res.shape\n",
    "labels = list((map(lambda x : 0 if x<4 else 1,twenty_train.target)))\n",
    "labels = np.array(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execKMeans(data):\n",
    "    print(82 * '_')\n",
    "    print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI')\n",
    "\n",
    "    model = bench_k_means(n_clusters, data, 'k-means++')\n",
    "    bench_k_means(n_clusters, data, 'random')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t36.68s\t7484\t0.457\t0.486\t0.471\t0.486\t0.457\n",
      "random   \t33.00s\t7484\t0.435\t0.465\t0.449\t0.463\t0.435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=10,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execKMeans(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3(a): LSI and NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSI Training Data ==> (7882, 1000)\n",
      "Model with 1 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.10s\t9\t0.018\t0.018\t0.018\t0.025\t0.018\n",
      "random   \t0.12s\t9\t0.018\t0.018\t0.018\t0.025\t0.018\n",
      "Model with 2 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.12s\t41\t0.419\t0.446\t0.432\t0.454\t0.419\n",
      "random   \t0.12s\t41\t0.417\t0.445\t0.430\t0.451\t0.417\n",
      "Model with 3 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.13s\t71\t0.412\t0.440\t0.425\t0.445\t0.412\n",
      "random   \t0.13s\t71\t0.412\t0.440\t0.425\t0.445\t0.412\n",
      "Model with 5 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.13s\t119\t0.410\t0.444\t0.427\t0.428\t0.410\n",
      "random   \t0.15s\t119\t0.409\t0.443\t0.425\t0.426\t0.409\n",
      "Model with 10 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.18s\t216\t0.429\t0.459\t0.444\t0.458\t0.429\n",
      "random   \t0.19s\t216\t0.429\t0.459\t0.443\t0.457\t0.429\n",
      "Model with 20 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.32s\t363\t0.421\t0.454\t0.437\t0.442\t0.421\n",
      "random   \t0.28s\t363\t0.422\t0.455\t0.438\t0.444\t0.422\n",
      "Model with 50 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.58s\t684\t0.428\t0.460\t0.443\t0.448\t0.427\n",
      "random   \t0.56s\t684\t0.418\t0.454\t0.435\t0.430\t0.418\n",
      "Model with 100 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.96s\t1090\t0.398\t0.441\t0.418\t0.395\t0.398\n",
      "random   \t0.93s\t1090\t0.450\t0.477\t0.463\t0.484\t0.450\n",
      "Model with 300 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t3.41s\t2172\t0.412\t0.449\t0.429\t0.422\t0.412\n",
      "random   \t2.79s\t2172\t0.378\t0.428\t0.401\t0.363\t0.378\n"
     ]
    }
   ],
   "source": [
    "load_from_previous = True\n",
    "#################### LSI ####################\n",
    "def getLSI(data,lsi=None,isTrain=True):\n",
    "    if(isTrain):\n",
    "        lsi = TruncatedSVD(n_components=1000, n_iter=7, random_state=42)\n",
    "        lsi_data = lsi.fit_transform(data)\n",
    "    else:\n",
    "        lsi_data = lsi.transform(data)\n",
    "    return lsi,lsi_data\n",
    "\n",
    "\n",
    "#################### LSI #######################\n",
    "if(load_from_previous):\n",
    "    model = load('lsi_model')\n",
    "    lsi = model['model']\n",
    "    lsi_train = model['train']\n",
    "else:    \n",
    "    lsi,lsi_train = getLSI(training_data)\n",
    "    save({'model':lsi,'train':lsi_train},'lsi_model')\n",
    "plt.plot(range(1,1001), lsi.explained_variance_ratio_ )\n",
    "plt.xlabel('No. of Components')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('Explained Variance for LSI')\n",
    "plt.show()\n",
    "\n",
    "lsi_models = []\n",
    "componentList = [1, 2, 3, 5 ,10, 20, 50, 100,300]\n",
    "print('Shape of LSI Training Data ==>', lsi_train.shape)\n",
    "for i in componentList:\n",
    "    print('Model with', i, 'components:')\n",
    "    lsi_models.append(execKMeans(lsi_train[:,:i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 1 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.09s\t0\t0.018\t0.018\t0.018\t0.025\t0.018\n",
      "random   \t0.11s\t0\t0.018\t0.018\t0.018\t0.025\t0.018\n",
      "Model with 2 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.09s\t1\t0.409\t0.440\t0.424\t0.434\t0.409\n",
      "random   \t0.09s\t1\t0.408\t0.440\t0.423\t0.433\t0.408\n",
      "Model with 3 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.13s\t3\t0.384\t0.417\t0.400\t0.406\t0.384\n",
      "random   \t0.13s\t3\t0.384\t0.417\t0.400\t0.406\t0.384\n",
      "Model with 5 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.12s\t8\t0.365\t0.388\t0.376\t0.410\t0.365\n",
      "random   \t0.16s\t8\t0.366\t0.389\t0.377\t0.412\t0.366\n",
      "Model with 10 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.18s\t18\t0.046\t0.057\t0.051\t0.043\t0.046\n",
      "random   \t0.20s\t18\t0.051\t0.061\t0.056\t0.051\t0.051\n",
      "Model with 20 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.26s\t35\t0.048\t0.165\t0.074\t0.008\t0.048\n",
      "random   \t0.25s\t36\t0.066\t0.101\t0.080\t0.045\t0.066\n",
      "Model with 50 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.51s\t81\t0.008\t0.027\t0.012\t0.003\t0.008\n",
      "random   \t0.47s\t81\t0.008\t0.027\t0.012\t0.003\t0.008\n",
      "Model with 100 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t0.88s\t159\t0.000\t0.000\t0.000\t-0.000\t-0.000\n",
      "random   \t0.81s\t159\t0.000\t0.000\t0.000\t-0.000\t-0.000\n",
      "Model with 300 components:\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\n",
      "k-means++\t2.31s\t420\t0.010\t0.048\t0.016\t0.002\t0.009\n",
      "random   \t2.10s\t420\t0.010\t0.048\t0.016\t0.002\t0.009\n"
     ]
    }
   ],
   "source": [
    "load_from_previous = False\n",
    "\n",
    "#################### NMF ####################\n",
    "def getNMF(data,nmf=None,isTrain=True, n_comp=50):\n",
    "    if(isTrain):\n",
    "        nmf = NMF(n_components=n_comp, init='random', random_state=42)\n",
    "        nmf_data = nmf.fit_transform(data)\n",
    "    else:\n",
    "        nmf_data = nmf.transform(data)\n",
    "    return nmf,nmf_data\n",
    "\n",
    "##################### NMF ####################\n",
    "nmf_models = []\n",
    "for i in componentList:\n",
    "    if(load_from_previous):\n",
    "        model = load('nmf_model_'+str(i))\n",
    "        nmf = model['model']\n",
    "        nmf_train = model['train']\n",
    "    else:    \n",
    "        nmf,nmf_train = getNMF(training_data, n_comp=i)\n",
    "        save({'model':nmf,'train':nmf_train},'nmf_model_'+str(i))\n",
    "        print('Model with', i, 'components:')\n",
    "        nmf_models.append(execKMeans(nmf_train[:,:i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
