{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import os\n",
    "from surprise import BaselineOnly\n",
    "from surprise import Dataset\n",
    "from surprise import Reader \n",
    "from sklearn.model_selection import cross_validate\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = {'gohawks' : 188136,'nfl' : 259024,'sb49' : 826951,'gopatriots' : 26232,'patriots' : 489713,'superbowl' : 1348767}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation(X, y, model_number, features):\n",
    "    error = []\n",
    "    rmse =[]\n",
    "    for train_ind, test_ind in KFold(n_splits=10).split(X):\n",
    "        X_train, X_test = X[train_ind], X[test_ind]\n",
    "        y_train, y_test = y[train_ind], y[test_ind]\n",
    "        if model_number==0:\n",
    "            modl = linear_model.Ridge(alpha = .5)\n",
    "            modl.fit(X_train,y_train)\n",
    "        elif model_number==1:\n",
    "            modl = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "            modl.fit(X_train,y_train)\n",
    "        elif model_number==2:\n",
    "            modl = MLPRegressor(activation='relu',solver='adam', alpha=1e-5,\n",
    "                           hidden_layer_sizes=(features, 1), random_state=1)\n",
    "            modl.fit(X_train,y_train)\n",
    "        elif model_number==3:\n",
    "            modl=RandomForestRegressor(max_depth=features/2, random_state=0)\n",
    "            modl.fit(X_train,y_train)\n",
    "        else:\n",
    "            modl=linear_model.LinearRegression()\n",
    "            modl.fit(X_train,y_train)\n",
    "        predction = modl.predict(X_test)\n",
    "        error.append(mean_absolute_error(y_test, predction))\n",
    "        rmse.append(sqrt(mean_squared_error(y_test, predction)))\n",
    "    return error, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_meanerror(X, y, model_number, features):\n",
    "    if model_number==0:\n",
    "        modl = linear_model.Ridge(alpha = .5)\n",
    "        modl.fit(X,y)\n",
    "    elif model_number==1:\n",
    "        modl = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "        modl.fit(X,y)\n",
    "    elif model_number==2:#non linear\n",
    "            modl = MLPRegressor(activation='relu',solver='adam', alpha=1e-5,\n",
    "                           hidden_layer_sizes=(features, 1), random_state=1)\n",
    "            modl.fit(X,y)\n",
    "    elif model_number==3:\n",
    "            modl=RandomForestRegressor(max_depth=features/2, random_state=0)\n",
    "            modl.fit(X,y)\n",
    "    else:\n",
    "        modl=linear_model.LinearRegression()\n",
    "        modl.fit(X,y)\n",
    "    predction = modl.predict(X)\n",
    "    error=mean_absolute_error(y, predction)\n",
    "    rmse=sqrt(mean_squared_error(y, predction))\n",
    "    return error,rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(htag, grp, model_number, features):  \n",
    "    number_tweets=hashtags[htag]\n",
    "    print(\"Data for \" + str(htag) + \" hashtag\")\n",
    "    #prepare to read each file\n",
    "    with open(\"tweet_data/\" + \"\".join('tweets_#' + htag + '.txt'), 'r') as fl:\n",
    "        datafr = pd.DataFrame(index=range(number_tweets), columns=['Time', 'tweetCount', 'retweetCount', 'followerSum', 'maxFollowers', 'location', 'impressionCount',\n",
    "            'favoriteCount','longTweets', 'rankingScore', 'userID'])\n",
    "        cnt = 0\n",
    "        for line in fl:\n",
    "            tweet = json.loads(line)\n",
    "            date = datetime.fromtimestamp(tweet['firstpost_date'])\n",
    "            datafr.set_value(cnt, 'Time', date)\n",
    "            datafr.set_value(cnt, 'tweetCount', 1)\n",
    "            datafr.set_value(cnt, 'retweetCount', tweet['metrics']['citations']['total'])\n",
    "            datafr.set_value(cnt, 'followerSum', tweet['author']['followers'])\n",
    "            datafr.set_value(cnt, 'maxFollowers', tweet['author']['followers'])\n",
    "            datafr.set_value(cnt, 'location', tweet['tweet']['user']['location'])\n",
    "            datafr.set_value(cnt, 'impressionCount', tweet['metrics']['impressions'])\n",
    "            datafr.set_value(cnt, 'favoriteCount', tweet['tweet']['favorite_count'])\n",
    "            datafr.set_value(cnt, 'longTweet', len(tweet.get('title')) > 100)\n",
    "            datafr.set_value(cnt, 'rankingScore', tweet.get('metrics').get('ranking_score'))\n",
    "            datafr.set_value(cnt, 'userID', tweet.get('tweet').get('user').get('id'))\n",
    "            cnt += 1\n",
    "\n",
    "        start_date=datetime(2015,2,1,8,0,0)\n",
    "        end_date=datetime(2015,2,1,20,0,0)\n",
    "        if prd==1:\n",
    "            df_start= datafr[datafr.Time<start_date]\n",
    "            print(\"Before Feb. 1, 8:00 a.m.\")\n",
    "            X,y=hourly_data(df_start,features)\n",
    "        elif prd==2:\n",
    "            df_mid = datafr[(datafr.Time>start_date) & (datafr.Time<end_date)]\n",
    "            print(\"Between Feb. 1, 8:00 a.m. and 8:00 p.m.\")\n",
    "            X,y=hourly_data(df_mid,features)\n",
    "        else:\n",
    "            df_end = datafr[datafr.Time>end_date]\n",
    "            print(\"After Feb. 1, 8:00 p.m\")\n",
    "            X,y=hourly_data(df_end,features)\n",
    "    \n",
    "    if model_number==0:\n",
    "        modl = linear_model.Ridge(alpha = .5)\n",
    "        modl.fit(X,y)\n",
    "    elif model_number==1:\n",
    "        modl = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "        modl.fit(X,y)\n",
    "    elif model_number==2:#non linear\n",
    "        modl = MLPRegressor(activation='relu',solver='adam', alpha=1e-5,\n",
    "            hidden_layer_sizes=(features, 1), random_state=1)\n",
    "        modl.fit(X,y)\n",
    "    elif model_number==3:\n",
    "            modl=RandomForestRegressor(max_depth=features/2, random_state=0)\n",
    "            modl.fit(X,y)\n",
    "    else:\n",
    "        modl=linear_model.LinearRegression()\n",
    "        modl.fit(X,y)\n",
    "    return modl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_data(datafr, features):\n",
    "    datafr = datafr.set_index('Time')\n",
    "    group_hour = datafr.groupby(pd.TimeGrouper(freq='60Min'))\n",
    "    X=np.ones((len(group_hour),features))\n",
    "    y=np.ones((len(group_hour),1))\n",
    "    for cnt,(interval,grp) in enumerate(group_hour):\n",
    "        y[cnt,0]=grp.tweetCount.sum()\n",
    "        tweetCount = grp.tweetCount.sum()\n",
    "        tweetCount = max(1, tweetCount)\n",
    "        X[cnt, 0] = interval.hour                 # Hour of the day\n",
    "        X[cnt, 1] = tweetCount                   # Number of tweets\n",
    "        X[cnt, 2] = grp.retweetCount.sum()      # Number of retweets\n",
    "        X[cnt, 3] = grp.followerSum.sum()       # Sum of followers in this interval\n",
    "        X[cnt, 4] = grp.maxFollowers.max()      # Max following in this interval\n",
    "        X[cnt, 5] = grp.location.value_counts().max()\n",
    "        X[cnt, 6] = grp.impressionCount.sum() / tweetCount   # Sum of impression count\n",
    "        X[cnt, 7] = grp.favoriteCount.sum() / tweetCount    # Sum of favorites\n",
    "        X[cnt, 8] = grp.longTweet.sum()         # Number of long tweets\n",
    "        X[cnt, 9] = grp.rankingScore.sum() / tweetCount\n",
    "        X[cnt, 10] = grp.userID.nunique()\n",
    "        \n",
    "    y=y[1:]\n",
    "    X=np.nan_to_num(X[:-1])    \n",
    "    return X, y\n",
    "\n",
    "def six_hour(datafr, features):\n",
    "    datafr = datafr.set_index('Time')\n",
    "    group_month = datafr.groupby(pd.TimeGrouper(freq='60Min'))\n",
    "    X=np.ones((len(group_month),features))\n",
    "    y=np.ones((len(group_month),1))\n",
    "    for cnt,(interval,grp) in enumerate(group_month):\n",
    "        y[cnt,0]=grp.tweetCount.sum()\n",
    "        tweetCount = grp.tweetCount.sum()\n",
    "        tweetCount = max(1, tweetCount)\n",
    "        X[cnt, 0] = interval.hour                 # Hour of the day\n",
    "        X[cnt, 1] = tweetCount                   # Number of tweets\n",
    "        X[cnt, 2] = grp.retweetCount.sum()      # Number of retweets\n",
    "        X[cnt, 3] = grp.followerSum.sum()       # Sum of followers in this interval\n",
    "        X[cnt, 4] = grp.maxFollowers.max()      # Max following in this interval\n",
    "        X[cnt, 5] = grp.location.value_counts().max()\n",
    "        X[cnt, 6] = grp.impressionCount.sum() / tweetCount   # Sum of impression count\n",
    "        X[cnt, 7] = grp.favoriteCount.sum() / tweetCount    # Sum of favorites\n",
    "        X[cnt, 8] = grp.longTweet.sum()         # Number of long tweets\n",
    "        X[cnt, 9] = grp.rankingScore.sum() / tweetCount\n",
    "        X[cnt, 10] = grp.userID.nunique()\n",
    "        \n",
    "    y=y[5:]\n",
    "    X=np.nan_to_num(X[:-5])    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression type: LinearRidge\n",
      "Data for gohawks hashtag\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "all_tweetCount = {}\n",
    "regrNum=[\"LinearRidge\", \"SVR\", \"NeuralNet\", \"RandomForest\", 'LinearReg']\n",
    "features=12#########CHANGE IT TO 9\n",
    "result={\"LinearRidge\":{}, \"SVR\":{}, \"NeuralNet\":{}, \"RandomForest\":{}, 'LinearReg':{}}\n",
    "def exec_regr(model_number):\n",
    "    dfstart_arr_mae=[]\n",
    "    dfmid_arr_mae=[]\n",
    "    dfend_arr_mae=[]\n",
    "    dfstart_arr_rmse=[]\n",
    "    dfmid_arr_rmse=[]\n",
    "    dfend_arr_rmse=[]\n",
    "    print(\"regression type: \"+str(regrNum[model_number]))\n",
    "    htags={}\n",
    "    for (htag, number_tweets) in hashtags.items():\n",
    "        htags[htag]={'Before':{'MAE':0, 'RMSE':0}, 'During':{'MAE':0, 'RMSE':0}, 'After':{'MAE':0, 'RMSE':0}} \n",
    "        print(\"Data for \" + str(htag) + \" hashtag\")\n",
    "        with open(\"tweet_data/\" + \"\".join('tweets_#' + htag + '.txt'), 'r', encoding=\"utf8\") as fl:\n",
    "            datafr = pd.DataFrame(index=range(number_tweets), columns=['Time', 'tweetCount', 'retweetCount', 'followerSum', 'maxFollowers', 'location', 'impressionCount',\n",
    "            'favoriteCount','longTweets', 'rankingScore', 'userID'])\n",
    "            users = {}\n",
    "            cnt = 0\n",
    "            for line in fl:\n",
    "                #load each line and \n",
    "                tweet = json.loads(line)\n",
    "                date = datetime.fromtimestamp(tweet['firstpost_date'])\n",
    "                #create each line to add to the dataframe\n",
    "                datafr.set_value(cnt, 'Time', date)\n",
    "                datafr.set_value(cnt, 'tweetCount', 1)\n",
    "                datafr.set_value(cnt, 'retweetCount', tweet['metrics']['citations']['total'])\n",
    "                datafr.set_value(cnt, 'followerSum', tweet['author']['followers'])\n",
    "                datafr.set_value(cnt, 'maxFollowers', tweet['author']['followers'])\n",
    "                datafr.set_value(cnt, 'location', tweet['tweet']['user']['location'])\n",
    "                datafr.set_value(cnt, 'impressionCount', tweet['metrics']['impressions'])\n",
    "                datafr.set_value(cnt, 'favoriteCount', tweet['tweet']['favorite_count'])\n",
    "                datafr.set_value(cnt, 'longTweet', len(tweet.get('title')) > 100)\n",
    "                datafr.set_value(cnt, 'rankingScore', tweet.get('metrics').get('ranking_score'))\n",
    "                datafr.set_value(cnt, 'userID', tweet.get('tweet').get('user').get('id'))\n",
    "                cnt += 1\n",
    "\n",
    "            start_date=datetime(2015,2,1,8,0,0)\n",
    "            end_date=datetime(2015,2,1,20,0,0)\n",
    "\n",
    "            df_start = datafr[datafr.Time<start_date]\n",
    "            print(\"Before Feb. 1, 8:00 a.m.\")\n",
    "            X,y=hourly_data(df_start,features)\n",
    "            mae, rmse = crossvalidation(X,y, model_number, features)\n",
    "            print(\"Mean absolute error: \", np.mean(mae))\n",
    "            print(\"RMSE: \", np.sqrt(np.mean(rmse)))\n",
    "            htags[htag]['Before']['MAE'] = np.mean(mae)\n",
    "            htags[htag]['Before']['RMSE'] = np.sqrt(np.mean(rmse))\n",
    "            result[regrNum[model_number]]=htags\n",
    "            dfstart_arr_mae.append(np.mean(mae))\n",
    "            dfstart_arr_rmse.append(np.sqrt(np.mean(rmse)))\n",
    "\n",
    "            df_mid = datafr[(datafr.Time>start_date) & (datafr.Time<end_date)]\n",
    "            print(\"Between Feb. 1, 8:00 a.m. and 8:00 p.m.\")\n",
    "            X,y=hourly_data(df_mid,features)\n",
    "            mae, rmse = crossvalidation(X,y, model_number, features)\n",
    "            htags[htag]['During']['MAE'] = np.mean(mae)\n",
    "            htags[htag]['During']['RMSE'] = np.sqrt(np.mean(rmse))\n",
    "            result[regrNum[model_number]]=htags\n",
    "            print(\"Mean absolute error: \", np.mean(mae))\n",
    "            print(\"RMSE: \", np.sqrt(np.mean(rmse)))\n",
    "            dfmid_arr_mae.append(np.mean(mae))\n",
    "            dfmid_arr_rmse.append(np.sqrt(np.mean(rmse)))\n",
    "\n",
    "            df_end = datafr[datafr.Time>end_date]\n",
    "            print(\"After Feb. 1, 8:00 p.m\")\n",
    "            X,y=hourly_data(df_end,features)\n",
    "            mae, rmse = crossvalidation(X,y, model_number, features)\n",
    "            htags[htag]['After']['MAE'] = np.mean(mae)\n",
    "            htags[htag]['After']['RMSE'] = np.sqrt(np.mean(rmse))\n",
    "            result[regrNum[model_number]]=htags\n",
    "            print(\"Mean absolute error: \", np.mean(mae))\n",
    "            print(\"RMSE: \", np.sqrt(np.mean(rmse)))\n",
    "            dfend_arr_mae.append(np.mean(mae))\n",
    "            dfend_arr_rmse.append(np.sqrt(np.mean(rmse)))\n",
    "#     print dfstart_arr_rmse, dfmid_arr_rmse, dfend_arr_rmse\n",
    "    print(dfstart_arr_mae, dfmid_arr_mae, dfend_arr_mae)\n",
    "    return min(min(dfstart_arr_mae), min(dfmid_arr_mae), min(dfend_arr_mae)) #min of all hashtags and all periods \n",
    "# best_model based on min MAE\n",
    "models=[]\n",
    "models.append(exec_regr(0))#linear ridge\n",
    "models.append(exec_regr(1))#svr\n",
    "models.append(exec_regr(2))#NN\n",
    "models.append(exec_regr(3))#random forest\n",
    "models.append(exec_regr(4))#linear reg\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-f7f351422480>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbestmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(models)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "best_model = models.index(min(models))\n",
    "#print(models)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for gohawks hashtag\n",
      "Data for nfl hashtag\n",
      "Data for sb49 hashtag\n",
      "Data for gopatriots hashtag\n",
      "Data for patriots hashtag\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-ecc42657550d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[0mbestrunRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbestmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-ecc42657550d>\u001b[0m in \u001b[0;36mbestrunRegression\u001b[1;34m(bestmodel)\u001b[0m\n\u001b[0;32m     14\u001b[0m             'favoriteCount','longTweets', 'rankingScore', 'userID'])\n\u001b[0;32m     15\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[1;31m#load each line and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mtweet_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def bestrunRegression(best_model):\n",
    "    flag=-1\n",
    "    for (htag, number_tweets) in hashtags.items():\n",
    "        flag+=1\n",
    "        print(\"Data for \" + str(htag) + \" hashtag\")\n",
    "        with open(\"tweet/\" + \"\".join('tweets_#' + htag + '.txt'), 'r', encoding='utf8') as fl:\n",
    "            #make a datafr that stores the values of interest\n",
    "            datafr1 = pd.DataFrame(index=range(number_tweets), columns=['Time', 'tweetCount', 'retweetCount', 'followerSum', 'maxFollowers', 'location', 'impressionCount',\n",
    "            'favoriteCount','longTweets', 'rankingScore', 'userID'])\n",
    "            cnt = 0\n",
    "            for line in fl:\n",
    "                #load each line and \n",
    "                tweet = json.loads(line)\n",
    "                date = datetime.fromtimestamp(tweet['firstpost_date'])\n",
    "                #create each line to add to the dataframe\n",
    "                datafr1.set_value(cnt, 'Time', date)\n",
    "                datafr1.set_value(cnt, 'tweetCount', 1)\n",
    "                datafr1.set_value(cnt, 'retweetCount', tweet['metrics']['citations']['total'])\n",
    "                datafr1.set_value(cnt, 'followerSum', tweet['author']['followers'])\n",
    "                datafr1.set_value(cnt, 'maxFollowers', tweet['author']['followers'])\n",
    "                datafr1.set_value(cnt, 'location', tweet['tweet']['user']['location'])\n",
    "                datafr1.set_value(cnt, 'impressionCount', tweet['metrics']['impressions'])\n",
    "                datafr1.set_value(cnt, 'favoriteCount', tweet['tweet']['favorite_count'])\n",
    "                datafr1.set_value(cnt, 'longTweet', len(tweet.get('title')) > 100)\n",
    "                datafr1.set_value(cnt, 'rankingScore', tweet.get('metrics').get('ranking_score'))\n",
    "                datafr1.set_value(cnt, 'userID', tweet.get('tweet').get('user').get('id'))\n",
    "                cnt += 1\n",
    "            if flag==0:\n",
    "                datafr = pd.concat([datafr1])\n",
    "            else: datafr = pd.concat([datafr, datafr1])\n",
    "    #     once all hashtags are in one data frame\n",
    "    #     print datafr.iloc[288135,1]\n",
    "    start_date=datetime(2015,2,1,8,0,0)\n",
    "    end_date=datetime(2015,2,1,20,0,0)\n",
    "\n",
    "    df_start = datafr[datafr.Time<start_date]\n",
    "    print(\"Before Feb. 1, 8:00 a.m.\")\n",
    "    X,y=hourly_data(df_start,features)\n",
    "    #WIDE DATA FORMAT HERE\n",
    "    mae, rmse = calc_meanerror(X,y, best_model,features)\n",
    "    print(\"Mean absolute error: \", mae)\n",
    "    print(\"RMSE: \", rmse)\n",
    "\n",
    "    df_mid = datafr[(datafr.Time>start_date) & (datafr.Time<end_date)]\n",
    "    print(\"Between Feb. 1, 8:00 a.m. and 8:00 p.m.\")\n",
    "    X,y=hourly_data(df_mid,features)\n",
    "    mae, rmse = calc_meanerror(X,y, best_model,features)\n",
    "    print(\"Mean absolute error: \", mae)\n",
    "    print(\"RMSE: \", rmse)\n",
    "\n",
    "    df_end = datafr[datafr.Time>end_date]\n",
    "    print(\"After Feb. 1, 8:00 p.m\")\n",
    "    X,y=hourly_data(df_end,features)\n",
    "    mae, rmse = calc_meanerror(X,y, best_model,features)\n",
    "    print(\"Mean absolute error: \", mae)\n",
    "    print(\"RMSE: \", rmse)\n",
    "    \n",
    "bestrunRegression(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tokenize(text, tofind):\n",
    "    text = re.sub(r'[^A-Za-z]', \" \", text)\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    text = re.sub(\"[,.-:/()?{}*$&\\;!@%]^><~\", \" \", text)\n",
    "    text = \"\".join(ch for ch in text if ord(ch) < 128)\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return findWord(tokens)\n",
    "\n",
    "def findWord(tokens, tofind):\n",
    "    if tofind in tokens:\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_data/sample10_period3.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-1adabe4eb0b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mgetHashtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sample10_period3.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-1adabe4eb0b7>\u001b[0m in \u001b[0;36mgetHashtag\u001b[1;34m(filename, idx)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetHashtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_data/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_data/sample10_period3.txt'"
     ]
    }
   ],
   "source": [
    "mapping={}\n",
    "def getHashtag(filename, idx):\n",
    "    mapping[idx]=\"\"\n",
    "    with open(\"test_data/\" + filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data= json.loads(line)\n",
    "#             print data\n",
    "            print(data['tweet']['user']['description'])\n",
    "            break\n",
    "getHashtag(\"sample10_period3.txt\", 1)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataConverttoWide(X, y):\n",
    "    mod_y= []\n",
    "    counter=0\n",
    "    features=11\n",
    "    mod_X= np.ones((1,features*6))#initialize mod_X as a row  of (5+1)*12 features\n",
    "    temp=np.ones((1,features))#initialize as a row  of 12 features\n",
    "    for x in X:\n",
    "        counter+=1\n",
    "#         print temp.shape, x.shape\n",
    "        temp = np.concatenate((temp, x.reshape(1,features)),axis=1)#temp will have (6+1)*12 features\n",
    "        if(counter==5):\n",
    "            mod_X= np.concatenate((mod_X, temp),axis=0)\n",
    "            counter=0\n",
    "            temp=np.ones((1,features))#initialize as a row  of 12 features\n",
    "#     final mod_X will remove first row as it was initialized and first 12 columns\n",
    "    mod_X=mod_X[1:,features:]\n",
    "#     create y\n",
    "#     print y.shape[0]\n",
    "#     print y\n",
    "    for idx in range(4, y.shape[0], 5):\n",
    "        mod_y.append(y[idx,0])\n",
    "    mod_y= np.array(mod_y)\n",
    "    print(mod_X.shape, mod_y.shape)\n",
    "    return mod_X, mod_y.reshape(mod_y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for gohawks hashtag\n",
      "Data for nfl hashtag\n",
      "Data for sb49 hashtag\n",
      "Data for gopatriots hashtag\n",
      "Data for patriots hashtag\n",
      "Data for superbowl hashtag\n",
      "Before Feb. 1, 8:00 a.m.\n",
      "Between Feb. 1, 8:00 a.m. and 8:00 p.m.\n",
      "After Feb. 1, 8:00 p.m\n"
     ]
    }
   ],
   "source": [
    "# datafor  1.5 b part\n",
    "def getAllData():\n",
    "    flag=-1\n",
    "    #     df=pd.DataFrame(index=range(1), columns=['Time', 'tweetCount', 'retweetCount', 'followerSum', 'maxFollowers', 'location', 'impressionCount',\n",
    "    #             'favoriteCount','longTweets'])\n",
    "\n",
    "    for (htag, num_tweets) in hashtags.items():\n",
    "        flag+=1\n",
    "        print(\"Data for \" + str(htag) + \" hashtag\")\n",
    "        #prepare to read each file\n",
    "        with open(\"tweet_data/\" + \"\".join('tweets_#' + htag + '.txt'), 'r', encoding=\"utf8\") as f:\n",
    "            #make a df that stores the values of interest\n",
    "            df1 = pd.DataFrame(index=range(num_tweets), columns=['Time', 'tweetCount', 'retweetCount', 'followerSum', 'maxFollowers', 'location', 'impressionCount',\n",
    "            'favoriteCount','longTweets', 'rankingScore', 'userID'])\n",
    "            counter = 0\n",
    "            for line in f:\n",
    "                #load each line and \n",
    "                tweet_data = json.loads(line)\n",
    "                date = datetime.fromtimestamp(tweet_data['firstpost_date'])\n",
    "                #create each line to add to the dataframe\n",
    "                df1.set_value(counter, 'Time', date)\n",
    "                df1.set_value(counter, 'tweetCount', 1)\n",
    "                df1.set_value(counter, 'retweetCount', tweet_data['metrics']['citations']['total'])\n",
    "                df1.set_value(counter, 'followerSum', tweet_data['author']['followers'])\n",
    "                df1.set_value(counter, 'maxFollowers', tweet_data['author']['followers'])\n",
    "                df1.set_value(counter, 'location', tweet_data['tweet']['user']['location'])\n",
    "                df1.set_value(counter, 'impressionCount', tweet_data['metrics']['impressions'])\n",
    "                df1.set_value(counter, 'favoriteCount', tweet_data['tweet']['favorite_count'])\n",
    "                df1.set_value(counter, 'longTweet', len(tweet_data.get('title')) > 100)\n",
    "                df1.set_value(counter, 'rankingScore', tweet_data.get('metrics').get('ranking_score'))\n",
    "                df1.set_value(counter, 'userID', tweet_data.get('tweet').get('user').get('id'))\n",
    "                counter += 1\n",
    "            if flag==0:\n",
    "                df = pd.concat([df1])\n",
    "            else: df = pd.concat([df, df1])\n",
    "    #     once all hashtags are in one data frame\n",
    "    #     print df.iloc[288135,1]\n",
    "    startdate=datetime(2015,2,1,8,0,0)\n",
    "    enddate=datetime(2015,2,1,20,0,0)\n",
    "    \n",
    "    print(\"Before Feb. 1, 8:00 a.m.\")\n",
    "    dfstart = df[df.Time<startdate]\n",
    "    print(\"Between Feb. 1, 8:00 a.m. and 8:00 p.m.\")\n",
    "    dfmid = df[(df.Time>startdate) & (df.Time<enddate)]\n",
    "    print(\"After Feb. 1, 8:00 p.m\")\n",
    "    dfend = df[df.Time>enddate]\n",
    "    \n",
    "    return dfstart, dfmid, dfend\n",
    "    \n",
    "dfstart, dfmid, dfend = getAllData()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3138814\n"
     ]
    }
   ],
   "source": [
    "print (dfstart.shape[0]+ dfmid.shape[0]+ dfend.shape[0] )\n",
    "# summ=0\n",
    "# # print dfstart.shape[0]\n",
    "# for htag, num_tweets in hashtags.iteritems():\n",
    "#     summ+=num_tweets\n",
    "    \n",
    "# print summ\n",
    "# print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "    \"sample1_period1.txt\": 730,\n",
    "    \"sample2_period2.txt\": 212273,\n",
    "    \"sample3_period3.txt\": 3638,\n",
    "    \"sample4_period1.txt\": 1646,\n",
    "    \"sample5_period1.txt\": 2059,\n",
    "    \"sample6_period2.txt\": 205554,\n",
    "    \"sample7_period3.txt\": 528,\n",
    "    \"sample8_period1.txt\": 229,#4 hour data given\n",
    "    \"sample9_period2.txt\": 11311,\n",
    "    \"sample10_period3.txt\": 365\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 12 into shape (1,11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-b04d133dac03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mhourly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfstart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataConverttoWide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlm_start\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-8c8212166c35>\u001b[0m in \u001b[0;36mdataConverttoWide\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#         print temp.shape, x.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#temp will have (6+1)*12 features\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mmod_X\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 12 into shape (1,11)"
     ]
    }
   ],
   "source": [
    "\n",
    "X,y =hourly(dfstart,features)\n",
    "X_start,y_start=dataConverttoWide(X, y)\n",
    "lm_start=RandomForestRegressor(max_depth=features/2, random_state=0)\n",
    "lm_start.fit(X_start,y_start)\n",
    "\n",
    "X,y =hourly(dfmid,features)\n",
    "X_mid,y_mid=dataConverttoWide(X, y)\n",
    "lm_mid=RandomForestRegressor(max_depth=features/2, random_state=0)\n",
    "lm_mid.fit(X_mid,y_mid)\n",
    "\n",
    "X,y =hourly(dfend,features)\n",
    "X_end,y_end=dataConverttoWide(X, y)\n",
    "lm_end=RandomForestRegressor(max_depth=features/2, random_state=0)\n",
    "lm_end.fit(X_end,y_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample4_period1.txt\n",
      "(1L, 55L) (1L,)\n",
      "(1L, 55L) (1L, 1L)\n",
      "Prediction error MAE: 1850.16333333\n",
      "Prediction error RMSE: 1850.16333333\n",
      "sample1_period1.txt\n",
      "(15L, 55L) (15L,)\n",
      "(15L, 55L) (15L, 1L)\n",
      "Prediction error MAE: 194.38\n",
      "Prediction error RMSE: 200.135302566\n",
      "sample2_period2.txt\n",
      "(4L, 55L) (4L,)\n",
      "(4L, 55L) (4L, 1L)\n",
      "Prediction error MAE: 20086.82\n",
      "Prediction error RMSE: 38944.2048272\n",
      "sample10_period3.txt\n",
      "(1L, 55L) (1L,)\n",
      "(1L, 55L) (1L, 1L)\n",
      "Prediction error MAE: 153.6\n",
      "Prediction error RMSE: 153.6\n",
      "sample9_period2.txt\n",
      "(3L, 55L) (3L,)\n",
      "(3L, 55L) (3L, 1L)\n",
      "Prediction error MAE: 510.593333333\n",
      "Prediction error RMSE: 669.38634395\n",
      "sample3_period3.txt\n",
      "(1L, 55L) (1L,)\n",
      "(1L, 55L) (1L, 1L)\n",
      "Prediction error MAE: 892.027619048\n",
      "Prediction error RMSE: 892.027619048\n",
      "sample7_period3.txt\n",
      "(1L, 55L) (1L,)\n",
      "(1L, 55L) (1L, 1L)\n",
      "Prediction error MAE: 110.216666667\n",
      "Prediction error RMSE: 110.216666667\n",
      "sample6_period2.txt\n",
      "(5L, 55L) (5L,)\n",
      "(5L, 55L) (5L, 1L)\n",
      "Prediction error MAE: 6680.73433333\n",
      "Prediction error RMSE: 14389.1406177\n",
      "sample8_period1.txt\n",
      "(0L, 55L) (0L,)\n",
      "(0L, 55L) (0L, 1L)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 55)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-aea3c3d0c974>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlm_mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlm_end\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Prediction error MAE:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Prediction error RMSE:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aditi\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimators_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aditi\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    355\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\aditi\\Anaconda2\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[0;32m    375\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[1;32mC:\\Users\\aditi\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    460\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[1;32m--> 462\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 55)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for (filename,records) in samples.iteritems():\n",
    "    temp=re.search('.*?(\\\\d+).*?(\\\\d+)', filename)\n",
    "#     hashtag=getHashtag(filename, temp.group(1))\n",
    "    groupp=temp.group(2)\n",
    "#     print hashtag, groupp\n",
    "    with open(\"test_data/\" + filename, 'r') as f:\n",
    "        print filename\n",
    "        df = pd.DataFrame(index=range(records), columns=['Time', 'tweetCount', 'retweetCount', 'followerSum', 'maxFollowers', 'location', 'impressionCount',\n",
    "            'favoriteCount','longTweets', 'rankingScore', 'userID'])\n",
    "        counter=0\n",
    "        for line in f:\n",
    "                #load each line and \n",
    "                test_data = json.loads(line)\n",
    "                date = datetime.fromtimestamp(test_data['citation_date'])\n",
    "                #create each line to add to the dataframe\n",
    "                df.set_value(counter, 'Time', date)\n",
    "                df.set_value(counter, 'tweetCount', 1)\n",
    "                df.set_value(counter, 'retweetCount', test_data['metrics']['citations']['total'])\n",
    "                df.set_value(counter, 'followerSum', test_data['author']['followers'])\n",
    "                df.set_value(counter, 'maxFollowers', test_data['author']['followers'])\n",
    "                df.set_value(counter, 'location', test_data['tweet']['user']['location'])\n",
    "                df.set_value(counter, 'impressionCount', test_data['metrics']['impressions'])\n",
    "                df.set_value(counter, 'favoriteCount', test_data['tweet']['favorite_count'])\n",
    "                df.set_value(counter, 'longTweet', test_data.get('title') > 100)\n",
    "                df.set_value(counter, 'rankingScore', test_data.get('metrics').get('ranking_score'))\n",
    "                df.set_value(counter, 'userID', test_data.get('tweet').get('user').get('id'))\n",
    "                counter += 1\n",
    "                \n",
    "        X,y=hourly(df,features)\n",
    "        X_test,y_test=dataConverttoWide(X, y)\n",
    "        print X_test.shape ,y_test.shape\n",
    "        if(groupp==1):\n",
    "            y_pred=lm_start.predict(X_test)\n",
    "        elif(groupp==2):\n",
    "            y_pred=lm_mid.predict(X_test)\n",
    "        else:\n",
    "            y_pred=lm_end.predict(X_test)\n",
    "        print \"Prediction error MAE:\", mean_absolute_error(y_test, y_pred)\n",
    "        print \"Prediction error RMSE:\", sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sample8_period1.txt\n",
      "(4L, 11L) (4L, 1L)\n"
     ]
    }
   ],
   "source": [
    "# tetsing sample8 file\n",
    "# records=229\n",
    "# with open(\"test_data/\" + \"sample8_period1.txt\", 'r') as f:\n",
    "#     print filename\n",
    "#     df = pd.DataFrame(index=range(records), columns=['Time', 'tweetCount', 'retweetCount', 'followerSum', 'maxFollowers', 'location', 'impressionCount',\n",
    "#         'favoriteCount','longTweets', 'rankingScore', 'userID'])\n",
    "#     counter=0\n",
    "#     for line in f:\n",
    "#             #load each line and \n",
    "#             test_data = json.loads(line)\n",
    "#             date = datetime.fromtimestamp(test_data['firstpost_date'])\n",
    "#             #create each line to add to the dataframe\n",
    "#             df.set_value(counter, 'Time', date)\n",
    "#             df.set_value(counter, 'tweetCount', 1)\n",
    "#             df.set_value(counter, 'retweetCount', test_data['metrics']['citations']['total'])\n",
    "#             df.set_value(counter, 'followerSum', test_data['author']['followers'])\n",
    "#             df.set_value(counter, 'maxFollowers', test_data['author']['followers'])\n",
    "#             df.set_value(counter, 'location', test_data['tweet']['user']['location'])\n",
    "#             df.set_value(counter, 'impressionCount', test_data['metrics']['impressions'])\n",
    "#             df.set_value(counter, 'favoriteCount', test_data['tweet']['favorite_count'])\n",
    "#             df.set_value(counter, 'longTweet', test_data.get('title') > 100)\n",
    "#             df.set_value(counter, 'rankingScore', test_data.get('metrics').get('ranking_score'))\n",
    "#             df.set_value(counter, 'userID', test_data.get('tweet').get('user').get('id'))\n",
    "#             counter += 1\n",
    "\n",
    "#     X,y=hourly(df,features)\n",
    "#     print X.shape, y.shape\n",
    "# #     X_test,y_test=dataConverttoWide(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[ 5030.3]\n"
     ]
    }
   ],
   "source": [
    "print y_test\n",
    "print y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
